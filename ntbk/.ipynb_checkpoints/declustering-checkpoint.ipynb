{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ab2a3ba-7cfc-4994-9503-8afc7d1acb51",
   "metadata": {},
   "source": [
    "## NND declustering technique\n",
    "\n",
    "https://github.com/florentaden/nnd_declustering/tree/master\n",
    "\n",
    "**Data formatting - transforming the columns for running the nnd algo**\n",
    "\n",
    "This code will transform the CSV data to correct any overflow hours and calculate a continuous _time_ column.\n",
    "\n",
    "Continuous time in the context of the _time_ column means that the time values represent a continuous sequence starting from 0 for the first event and increasing in a uniform manner. This is typically measured in seconds, but it could be in other units such as minutes, hours, or days, depending on the context. The key point is that it represents the elapsed time from the first event to subsequent events, creating a continuous timeline.\n",
    "\n",
    "To clarify, here is how the continuous _time_ column is calculated and what it represents:\n",
    "\n",
    "**1. Start Time**: The first event in the dataset is assigned a time of 0.\n",
    "\n",
    "**2. Elapsed Time**: Each subsequent event's time is calculated as the difference in seconds (or another unit) from the first event's time.\n",
    "\n",
    "**3. Uniform Increments**: The _time_ values increase uniformly, providing a continuous measure of time elapsed since the first event\n",
    "\n",
    "This allows for easy comparison and analysis of events in a temporal sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "de86f3e4-36cc-42c5-9c07-0a0f423b3bdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7l/z17fky090gggg0_sw6p28lnr0000gp/T/ipykernel_49824/1603114636.py:19: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  csv_data['DATE'] = pd.to_datetime(csv_data['DATE'])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>hour</th>\n",
       "      <th>minute</th>\n",
       "      <th>seconde</th>\n",
       "      <th>time</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>depth</th>\n",
       "      <th>magnitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2006</td>\n",
       "      <td>8</td>\n",
       "      <td>31</td>\n",
       "      <td>14</td>\n",
       "      <td>57</td>\n",
       "      <td>30.0</td>\n",
       "      <td>8.731274e+08</td>\n",
       "      <td>27.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2014</td>\n",
       "      <td>3</td>\n",
       "      <td>28</td>\n",
       "      <td>10</td>\n",
       "      <td>44</td>\n",
       "      <td>48.0</td>\n",
       "      <td>1.112095e+09</td>\n",
       "      <td>25.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>6.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2026</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>19</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.507280e+09</td>\n",
       "      <td>26.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>6.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2026</td>\n",
       "      <td>5</td>\n",
       "      <td>29</td>\n",
       "      <td>22</td>\n",
       "      <td>37</td>\n",
       "      <td>32.0</td>\n",
       "      <td>1.496185e+09</td>\n",
       "      <td>15.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>5.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2027</td>\n",
       "      <td>3</td>\n",
       "      <td>15</td>\n",
       "      <td>16</td>\n",
       "      <td>56</td>\n",
       "      <td>32.0</td>\n",
       "      <td>1.521221e+09</td>\n",
       "      <td>24.5</td>\n",
       "      <td>95.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>6.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    year  month  day  hour  minute  seconde          time  latitude  \\\n",
       "1   2006      8   31    14      57     30.0  8.731274e+08      27.0   \n",
       "6   2014      3   28    10      44     48.0  1.112095e+09      25.0   \n",
       "12  2026     10    5     8      19     10.0  1.507280e+09      26.0   \n",
       "13  2026      5   29    22      37     32.0  1.496185e+09      15.0   \n",
       "14  2027      3   15    16      56     32.0  1.521221e+09      24.5   \n",
       "\n",
       "    longitude  depth  magnitude  \n",
       "1        97.0  100.0        7.0  \n",
       "6        99.0  100.0        6.9  \n",
       "12       97.0   80.0        6.2  \n",
       "13       92.0   35.0        5.6  \n",
       "14       95.0  130.0        6.5  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV data\n",
    "csv_file_path = '/Users/zuhair/Library/CloudStorage/OneDrive-NanyangTechnologicalUniversity/Research/current-projects/strain-rate-seismicity/data/Cleaned_ISC_Data.csv'\n",
    "csv_data = pd.read_csv(csv_file_path)\n",
    "\n",
    "# Correct overflow hours by converting them into additional days using pd.to_datetime\n",
    "def correct_datetime(row):\n",
    "    # Combine date and time parts\n",
    "    datetime_str = f\"{int(row['year'])}-{int(row['month'])}-{int(row['day'])} {int(row['hour'])}:{int(row['minute'])}:{row['seconde']}\"\n",
    "    try:\n",
    "        corrected_datetime = pd.to_datetime(datetime_str, errors='coerce')\n",
    "    except ValueError:\n",
    "        corrected_datetime = pd.NaT  # Handle invalid dates\n",
    "    \n",
    "    return corrected_datetime\n",
    "\n",
    "# Extract year, month, day from the DATE column\n",
    "csv_data['DATE'] = pd.to_datetime(csv_data['DATE'])\n",
    "csv_data['year'] = csv_data['DATE'].dt.year\n",
    "csv_data['month'] = csv_data['DATE'].dt.month\n",
    "csv_data['day'] = csv_data['DATE'].dt.day\n",
    "\n",
    "# Extract hour, minute, and second from the TIME column\n",
    "time_parts = csv_data['TIME'].str.split(':', expand=True)\n",
    "csv_data['hour'] = time_parts[0].astype(float)\n",
    "csv_data['minute'] = time_parts[1].astype(float)\n",
    "csv_data['seconde'] = time_parts[2].astype(float)\n",
    "\n",
    "# Apply the correction function\n",
    "csv_data['corrected_datetime'] = csv_data.apply(correct_datetime, axis=1)\n",
    "\n",
    "# Drop rows with invalid dates\n",
    "csv_data = csv_data.dropna(subset=['corrected_datetime'])\n",
    "\n",
    "# Calculate the elapsed time in seconds from the first event\n",
    "csv_data['time'] = (csv_data['corrected_datetime'] - csv_data['corrected_datetime'].min()).dt.total_seconds()\n",
    "\n",
    "# Extract year, month, day, hour, minute, and second from the corrected datetime\n",
    "csv_data['year'] = csv_data['corrected_datetime'].dt.year\n",
    "csv_data['month'] = csv_data['corrected_datetime'].dt.month\n",
    "csv_data['day'] = csv_data['corrected_datetime'].dt.day\n",
    "csv_data['hour'] = csv_data['corrected_datetime'].dt.hour\n",
    "csv_data['minute'] = csv_data['corrected_datetime'].dt.minute\n",
    "csv_data['seconde'] = csv_data['corrected_datetime'].dt.second + csv_data['corrected_datetime'].dt.microsecond / 1e6\n",
    "\n",
    "# Rename columns to match the Southern California Earthquake Data structure\n",
    "csv_data.rename(columns={\n",
    "    'LAT': 'latitude',\n",
    "    'LON': 'longitude',\n",
    "    'DEPTH': 'depth',\n",
    "    'MAG': 'magnitude'\n",
    "}, inplace=True)\n",
    "\n",
    "# Select and reorder the columns to match the target structure\n",
    "target_columns_with_corrected_time = ['year', 'month', 'day', 'hour', 'minute', 'seconde', 'time', 'latitude', 'longitude', 'depth', 'magnitude']\n",
    "transformed_csv_data_with_corrected_time = csv_data[target_columns_with_corrected_time]\n",
    "\n",
    "# Display the updated DataFrame to the user\n",
    "# import ace_tools as tools; tools.display_dataframe_to_user(name=\"Corrected CSV Data with Continuous Time Column\", dataframe=transformed_csv_data_with_corrected_time)\n",
    "\n",
    "transformed_csv_data_with_corrected_time.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d62798b7-af80-48c5-9d81-c95cb0e3e585",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned data saved to /Users/zuhair/Library/CloudStorage/OneDrive-NanyangTechnologicalUniversity/Research/current-projects/strain-rate-seismicity/data/transformed_cleaned_data.csv\n"
     ]
    }
   ],
   "source": [
    "# Save the cleaned DataFrame to a new CSV file\n",
    "output_file_path = '/Users/zuhair/Library/CloudStorage/OneDrive-NanyangTechnologicalUniversity/Research/current-projects/strain-rate-seismicity/data/transformed_cleaned_data.csv'\n",
    "\n",
    "transformed_csv_data_with_corrected_time.to_csv(output_file_path, index=False)\n",
    "\n",
    "print(f\"Cleaned data saved to {output_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f455606c-2513-4345-bff1-924c58f0c27a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the transformed data\n",
    "file_path = '/Users/zuhair/Library/CloudStorage/OneDrive-NanyangTechnologicalUniversity/Research/current-projects/strain-rate-seismicity/data/transformed_cleaned_data.csv'\n",
    "transformed_data = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "902bb65c-eeb8-4516-ad5a-acfedaad2ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from mpi4py import MPI\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize MPI\n",
    "doublesize = MPI.DOUBLE.Get_size()\n",
    "\n",
    "# All CPUs communicator\n",
    "ALL_COMM = MPI.COMM_WORLD\n",
    "rank, size = ALL_COMM.Get_rank(), ALL_COMM.Get_size()\n",
    "\n",
    "# All CPUs inside node communicator\n",
    "NODE_COMM = ALL_COMM.Split_type(MPI.COMM_TYPE_SHARED)\n",
    "node_rank, node_size = NODE_COMM.rank, NODE_COMM.size\n",
    "\n",
    "# Get one relay node per node\n",
    "value = (rank, node_rank)\n",
    "values = np.array(ALL_COMM.allgather(value))\n",
    "relay_ranks = values[values[:, 1] == 0, 0]\n",
    "group = ALL_COMM.group.Incl(relay_ranks)\n",
    "RELAY_COMM = ALL_COMM.Create_group(group)\n",
    "\n",
    "if rank in relay_ranks:\n",
    "    boss_rank, boss_size = RELAY_COMM.rank, RELAY_COMM.size\n",
    "\n",
    "# Parameters\n",
    "b_value = 1\n",
    "fractal_dimension = 1.6\n",
    "q = 0.5\n",
    "\n",
    "# Catalog name\n",
    "catalog_name = 'transformed_cleaned_data'\n",
    "output_name = catalog_name + '_nnd'\n",
    "\n",
    "# Read catalog\n",
    "catalog_df = transformed_data\n",
    "catalog_df = catalog_df[catalog_df.magnitude >= 2]\n",
    "catalog_df = catalog_df.reindex(['year', 'month', 'day', 'hour', 'minute',\n",
    "    'seconde', 'time', 'latitude', 'longitude', 'depth', 'magnitude'], axis=1)\n",
    "catalog = np.array(catalog_df)\n",
    "\n",
    "# Apply declustering and give the children dataframe\n",
    "children_df = catalog_df[1:]\n",
    "number_of_children, number_of_columns = children_df.shape\n",
    "\n",
    "# Shared children array (output)\n",
    "number_of_points = number_of_children * (number_of_columns + 5)\n",
    "nbytes = doublesize * number_of_points\n",
    "win = MPI.Win.Allocate_shared(nbytes if node_rank == 0 else 0, doublesize,\n",
    "    comm=NODE_COMM)\n",
    "buf, itemsize = win.Shared_query(0)\n",
    "assert itemsize == MPI.DOUBLE.Get_size()\n",
    "buf = np.array(buf, dtype='B', copy=False)\n",
    "children = np.ndarray(buffer=buf, dtype='d', shape=(number_of_children,\n",
    "    number_of_columns+5))\n",
    "\n",
    "if node_rank == 0:\n",
    "    children.fill(0)\n",
    "    children[:, :number_of_columns] = np.array(children_df)\n",
    "\n",
    "ALL_COMM.Barrier()\n",
    "\n",
    "for j, child in list(enumerate(children))[rank::size]:  # for each event\n",
    "    # Potential parents\n",
    "    k = (catalog[:, 6] < child[6])\n",
    "    parents = np.zeros((k.sum(), number_of_columns+5))\n",
    "    parents[:, :number_of_columns] = catalog[k]\n",
    "\n",
    "    # Compute temporal distance with all parents\n",
    "    parents[:, 11] = child[6] - parents[:, 6]\n",
    "    parents[:, 11] = parents[:, 11]*10**(-q*b_value*parents[:, 10])\n",
    "\n",
    "    # Compute physical distance with all parents\n",
    "    parents[:, 12] = np.sqrt((child[8]-parents[:, 8])**2 + \\\n",
    "                 (child[7]-parents[:, 7])**2)\n",
    "    parents[:, 12] = (parents[:, 12]**fractal_dimension)*10**(\n",
    "        (q-1)*b_value*parents[:, 10])\n",
    "\n",
    "    # Compute nearest_neighbor_distance metric with all parents\n",
    "    parents[:, 13] = parents[:, 11]*parents[:, 12]\n",
    "\n",
    "    nearest_neighbor = np.argmin(parents[:, 13])\n",
    "    children[j, 12] = parents[nearest_neighbor, 12]\n",
    "    children[j, 11] = parents[nearest_neighbor, 11]\n",
    "    children[j, 13] = parents[nearest_neighbor, 13]\n",
    "    children[j, 14] = parents[nearest_neighbor, 10]\n",
    "    children[j, 15] = nearest_neighbor\n",
    "\n",
    "ALL_COMM.Barrier()\n",
    "\n",
    "if rank in relay_ranks:\n",
    "    if rank == 0:\n",
    "        target = np.zeros_like(children)\n",
    "    else:\n",
    "        target = None\n",
    "\n",
    "    RELAY_COMM.Reduce([children, MPI.DOUBLE], [target, MPI.DOUBLE], op=MPI.SUM,\n",
    "        root=0)\n",
    "\n",
    "if rank == 0:\n",
    "    columns = ['year', 'month', 'day', 'hour', 'minute', 'seconde', 'time',\n",
    "              'latitude', 'longitude', 'depth', 'magnitude',\n",
    "              'Tij', 'Rij', 'Nij', 'parent_magnitude', 'neighbor']\n",
    "    children_df = pd.DataFrame(target, columns=columns)\n",
    "    children_df.to_hdf(output_name + '.h5', 'table')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
