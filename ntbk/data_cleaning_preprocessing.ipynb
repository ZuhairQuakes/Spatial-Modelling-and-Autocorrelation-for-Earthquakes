{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af21f03f-a464-41f2-9443-d209f8ecf07b",
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0xca in position 88: invalid continuation byte",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Read the raw data\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(file_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[0;32m----> 8\u001b[0m     raw_data \u001b[38;5;241m=\u001b[39m file\u001b[38;5;241m.\u001b[39mreadlines()\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Display the first few lines of the raw data to understand its structure\u001b[39;00m\n\u001b[1;32m     11\u001b[0m raw_data[:\u001b[38;5;241m10\u001b[39m]\n",
      "File \u001b[0;32m<frozen codecs>:322\u001b[0m, in \u001b[0;36mdecode\u001b[0;34m(self, input, final)\u001b[0m\n",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0xca in position 88: invalid continuation byte"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the uploaded CSV file\n",
    "file_path = '../data/raw_ISC_catalog.csv'\n",
    "\n",
    "# Read the raw data\n",
    "with open(file_path, 'r') as file:\n",
    "    raw_data = file.readlines()\n",
    "\n",
    "# Display the first few lines of the raw data to understand its structure\n",
    "raw_data[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c6c2142-8450-496e-b0c0-a0827984e30e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['International Seismological Centre\\n',\n",
       " 'ISC: Event catalogue\\n',\n",
       " '\\n',\n",
       " 'Any use of data from the ISCÊshould be cited. The correct format for citations may be found on ourÊcitation page.\\n',\n",
       " '\\n',\n",
       " '\"Once the search has completed, a compressedÊKML fileÊwill be available to view the results inÊGoogle Earth.\"\\n',\n",
       " '\\n',\n",
       " 'Make an event map\\n',\n",
       " '\\n',\n",
       " 'Search summary:\\n']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Attempt to read the file with a different encoding\n",
    "with open(file_path, 'r', encoding='latin1') as file:\n",
    "    raw_data = file.readlines()\n",
    "\n",
    "# Display the first few lines of the raw data to understand its structure\n",
    "raw_data[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c317563-8ea8-4ebd-99e2-226136e6def4",
   "metadata": {},
   "source": [
    "The file appears to contain introductory text and metadata that needs to be cleaned before extracting the relevant earthquake data. I'll clean up the file and extract the data into a structured format.\n",
    "\n",
    "Let's start by identifying and isolating the actual data lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8e2a65e4-59d6-4462-9097-2ffef3e7490f",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not NoneType",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Extract the header and data lines\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m header_line \u001b[38;5;241m=\u001b[39m raw_data[data_start_index]\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[1;32m     10\u001b[0m data_lines \u001b[38;5;241m=\u001b[39m raw_data[data_start_index \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m:]\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Clean and split the header and data lines into columns\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not NoneType"
     ]
    }
   ],
   "source": [
    "# Identify the start of the actual data by finding the first line with data columns\n",
    "data_start_index = None\n",
    "for index, line in enumerate(raw_data):\n",
    "    if 'Date' in line and 'Time' in line and 'Latitude' in line:\n",
    "        data_start_index = index\n",
    "        break\n",
    "\n",
    "# Extract the header and data lines\n",
    "header_line = raw_data[data_start_index].strip()\n",
    "data_lines = raw_data[data_start_index + 1:]\n",
    "\n",
    "# Clean and split the header and data lines into columns\n",
    "header = header_line.split(',')\n",
    "data = [line.strip().split(',') for line in data_lines]\n",
    "\n",
    "# Create a DataFrame from the extracted data\n",
    "earthquake_data = pd.DataFrame(data, columns=header)\n",
    "\n",
    "# Display the first few rows of the cleaned DataFrame\n",
    "import ace_tools as tools; tools.display_dataframe_to_user(name=\"Cleaned Earthquake Data\", dataframe=earthquake_data)\n",
    "earthquake_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6973e61-398d-462b-a602-13ca9353ec0d",
   "metadata": {},
   "source": [
    "It seems that the code did not find the correct header line, resulting in a NoneType error. I'll adjust the code to better identify and extract the data columns.\n",
    "\n",
    "Let's reprocess the file and look for the data header line more carefully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a35129f9-30b8-4b56-83c8-9f2cb2261d7b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'ace_tools'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 24\u001b[0m\n\u001b[1;32m     21\u001b[0m     earthquake_data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame()\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Display the first few rows of the cleaned DataFrame\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mace_tools\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtools\u001b[39;00m; tools\u001b[38;5;241m.\u001b[39mdisplay_dataframe_to_user(name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCleaned Earthquake Data\u001b[39m\u001b[38;5;124m\"\u001b[39m, dataframe\u001b[38;5;241m=\u001b[39mearthquake_data)\n\u001b[1;32m     25\u001b[0m earthquake_data\u001b[38;5;241m.\u001b[39mhead()\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'ace_tools'"
     ]
    }
   ],
   "source": [
    "# Identify the start of the actual data by finding the first line with data columns\n",
    "data_start_index = None\n",
    "for index, line in enumerate(raw_data):\n",
    "    if 'Date' in line and 'Time' in line and 'Latitude' in line:\n",
    "        data_start_index = index\n",
    "        break\n",
    "\n",
    "# Check if the header line was found\n",
    "if data_start_index is not None:\n",
    "    # Extract the header and data lines\n",
    "    header_line = raw_data[data_start_index].strip()\n",
    "    data_lines = raw_data[data_start_index + 1:]\n",
    "\n",
    "    # Clean and split the header and data lines into columns\n",
    "    header = header_line.split(',')\n",
    "    data = [line.strip().split(',') for line in data_lines]\n",
    "\n",
    "    # Create a DataFrame from the extracted data\n",
    "    earthquake_data = pd.DataFrame(data, columns=header)\n",
    "else:\n",
    "    earthquake_data = pd.DataFrame()\n",
    "\n",
    "# Display the first few rows of the cleaned DataFrame\n",
    "import ace_tools as tools; tools.display_dataframe_to_user(name=\"Cleaned Earthquake Data\", dataframe=earthquake_data)\n",
    "earthquake_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d0384e-234f-40f0-bad7-0a267edb1d79",
   "metadata": {},
   "source": [
    "It appears that the data header line wasn't found correctly. Let's manually inspect the raw data to determine the correct approach for extracting the actual data.\n",
    "\n",
    "Here are the first 50 lines of the raw data for review:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ffcdb456-d710-4b8c-b445-a5245b743f41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'International Seismological Centre\\nISC: Event catalogue\\n\\nAny use of data from the ISCÊshould be cited. The correct format for citations may be found on ourÊcitation page.\\n\\n\"Once the search has completed, a compressedÊKML fileÊwill be available to view the results inÊGoogle Earth.\"\\n\\nMake an event map\\n\\nSearch summary:\\nDatabase: Reviewed ISC Bulletin\\nSearch type: Rectangular search\\nLatitude range: 15 to 28\\nLongitude range: 90 to 100\\nStart date: 1600-05-01 00:00:00\\nEnd date: 2022-06-01 00:00:00\\nEvents found: 8602\\nDate of search: Mon May 27 04:38:07 2024 GMT\\n\\nAgencies whose data contributed towards the results of this search are listed at the bottom of the page.\\n\\nPlease note:\\n\\nData on or after 2022/06/01 have not been reviewed by the ISC.\\n\\nDATA_TYPE EVENT_CATALOGUE\\nReviewed ISC Bulletin\\n----EVENT-----|-----------------------ORIGIN (PRIME HYPOCENTRE)----------------|------MAGNITUDES-----...\\n\"  EVENTID,TYPE,AUTHOR   ,DATE      ,TIME       ,LAT     ,LON      ,DEPTH,DEPFIX,AUTHOR   ,TYPE  ,MAG \"\\n\" 16957826,ke  ,ISC      ,1904-08-30,11:43:37.59, 25.5166,  97.0211, 15.0,TRUE  ,ABE1     ,mB    , 7.6,AN2      ,Ms    , 6.8,BJI      ,Ms    , 6.8\"\\n\"   914281,ke  ,GUTE     ,1906-08-31,14:57:30   , 27.0000,  97.0000,100.0,      ,PAS      ,MS    , 7.0,ABE1     ,mB    , 6.7,GUTE     ,UK    , 7.0,B&D      ,UK    , 7.0\"\\n\" 16957984,ke  ,ISC      ,1908-02-09,18:13:17.58, 29.2781, 100.6556, 15.0,TRUE  ,ISC      ,MS    , 6.7,BJI      ,Ms    , 7.1,B&D      ,UK    , 7.1\"\\n\" 16958007,ke  ,ISC      ,1908-12-12,12:54:57.27, 26.9477,  96.7734, 15.0,TRUE  ,ISC      ,MS    , 7.0,PAS      ,MS    , 7.5,ABE1     ,mB    , 7.6,AN2      ,Ms    , 7.0,BJI      ,Ms    , 7.0,GUTE     ,UK    , 7.5\"\\n\" 16958191,ke  ,ISC      ,1912-05-23,02:24:04.06, 21.0361,  96.7366, 15.0,TRUE  ,ISC      ,MS    , 7.5,PAS      ,MS    , 8.0,ABE1     ,mB    , 7.3,AN2      ,Ms    , 7.7,P&S      ,Mw    , 7.7,GUTE     ,UK    , 8.0\"\\n\"   914027,ke  ,ISS      ,1913-01-08,19:17:03   , 22.5000, 117.5000,     ,      ,BJI      ,Ms    , 6.5\"\\n\"   914108,ke  ,ISC      ,1913-12-21,15:37:56.50, 24.2449, 102.2274, 15.0,TRUE  ,ISC      ,MS    , 6.6,P&S      ,Ms    , 7.1,AN1      ,Ms    , 7.2,P&S      ,Mw    , 7.2,B&D      ,UK    , 7.2\"\\n\"   913976,ke  ,GUTE     ,1914-03-28,10:44:48   , 25.0000,  99.0000,100.0,      ,PAS      ,MS    , 6.9,BJI      ,Ms    , 6.6,GUTE     ,UK    , 6.7\"\\n\"   913954,ke  ,ISC      ,1915-12-03,02:39:24.54, 27.6997,  91.6401, 15.0,TRUE  ,ISC      ,MS    , 6.5,AN1      ,Ms    , 7.0,BJI      ,Ms    , 7.0,B&D      ,UK    , 7.1\"\\n\"   913446,ke  ,ISS      ,1917-04-12,02:54:35   , 18.0000,  97.0000,     ,      ,         ,      ,\"\\n\"   913196,ke  ,ISC      ,1918-07-08,10:22:08.90, 24.5990,  90.8528, 15.0,TRUE  ,ISC      ,MS    , 7.2,PAS      ,MS    , 7.6,ABE1     ,mB    , 7.6,P&S      ,Ms    , 7.4,ABE1     ,Ms    , 7.6,P&S      ,Mw    , 7.5,B&D      ,UK    , 7.6\"\\n\"   912923,ke  ,ISS      ,1919-09-08,04:08:00   , 18.0000,  97.0000,     ,      ,         ,      ,\"\\n\"   912579,ke  ,ISS      ,1920-08-15,06:59:08   , 22.2000,  93.2000,     ,      ,         ,      ,\"\\n\"   912194,ke  ,ISS      ,1921-03-30,10:26:30   , 22.2000,  93.2000,     ,      ,         ,      ,\"\\n\"   912311,ke  ,ISS      ,1921-09-12,05:09:48   , 18.0000,  97.0000,     ,      ,         ,      ,\"\\n\"   911885,ke  ,ISC      ,1922-05-02,11:10:53.44, 20.0485,  98.2534, 35.0,TRUE  ,ISC      ,MS    , 6.7,\"\\n\"   912124,ke  ,ISS      ,1922-12-24,00:06:26   , 21.0000,  97.0000,     ,      ,         ,      ,\"\\n\"   911420,ke  ,ISC      ,1923-06-22,06:44:40.31, 22.8451,  98.7111, 25.0,TRUE  ,ISC      ,MS    , 7.3,PAS      ,MS    , 7.3,ABE1     ,mB    , 6.3,P&S      ,Ms    , 7.1,ABE1     ,Ms    , 7.3,P&S      ,Mw    , 7.2,B&D      ,UK    , 7.3\"\\n\"   911421,ke  ,ISS      ,1923-06-22,12:06:04   , 22.7000,  99.0000,     ,      ,         ,      ,\"\\n\"   911495,ke  ,ISS      ,1923-08-10,15:58:06   , 22.6000,  93.4000,     ,      ,         ,      ,\"\\n\"   911634,ke  ,ISC      ,1923-09-09,22:03:46.70, 24.9372,  90.7207, 15.0,TRUE  ,ISC      ,MS    , 6.9,PAS      ,MS    , 7.1,ABE1     ,Ms    , 7.1,GUTE     ,UK    , 7.1,B&D      ,UK    , 7.1\"\\n'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{''.join(raw_data[:50])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b5d630da-5928-4843-8ef8-024955cd6922",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Database: Reviewed ISC Bulletin\\n',\n",
       " 'Search type: Rectangular search\\n',\n",
       " 'Latitude range: 15 to 28\\n',\n",
       " 'Longitude range: 90 to 100\\n',\n",
       " 'Start date: 1600-05-01 00:00:00\\n',\n",
       " 'End date: 2022-06-01 00:00:00\\n',\n",
       " 'Events found: 8602\\n',\n",
       " 'Date of search: Mon May 27 04:38:07 2024 GMT\\n',\n",
       " '\\n',\n",
       " 'Agencies whose data contributed towards the results of this search are listed at the bottom of the page.\\n',\n",
       " '\\n',\n",
       " 'Please note:\\n',\n",
       " '\\n',\n",
       " 'Data on or after 2022/06/01 have not been reviewed by the ISC.\\n',\n",
       " '\\n',\n",
       " 'DATA_TYPE EVENT_CATALOGUE\\n',\n",
       " 'Reviewed ISC Bulletin\\n',\n",
       " '----EVENT-----|-----------------------ORIGIN (PRIME HYPOCENTRE)----------------|------MAGNITUDES-----...\\n',\n",
       " '\"  EVENTID,TYPE,AUTHOR   ,DATE      ,TIME       ,LAT     ,LON      ,DEPTH,DEPFIX,AUTHOR   ,TYPE  ,MAG \"\\n',\n",
       " '\" 16957826,ke  ,ISC      ,1904-08-30,11:43:37.59, 25.5166,  97.0211, 15.0,TRUE  ,ABE1     ,mB    , 7.6,AN2      ,Ms    , 6.8,BJI      ,Ms    , 6.8\"\\n',\n",
       " '\"   914281,ke  ,GUTE     ,1906-08-31,14:57:30   , 27.0000,  97.0000,100.0,      ,PAS      ,MS    , 7.0,ABE1     ,mB    , 6.7,GUTE     ,UK    , 7.0,B&D      ,UK    , 7.0\"\\n',\n",
       " '\" 16957984,ke  ,ISC      ,1908-02-09,18:13:17.58, 29.2781, 100.6556, 15.0,TRUE  ,ISC      ,MS    , 6.7,BJI      ,Ms    , 7.1,B&D      ,UK    , 7.1\"\\n',\n",
       " '\" 16958007,ke  ,ISC      ,1908-12-12,12:54:57.27, 26.9477,  96.7734, 15.0,TRUE  ,ISC      ,MS    , 7.0,PAS      ,MS    , 7.5,ABE1     ,mB    , 7.6,AN2      ,Ms    , 7.0,BJI      ,Ms    , 7.0,GUTE     ,UK    , 7.5\"\\n',\n",
       " '\" 16958191,ke  ,ISC      ,1912-05-23,02:24:04.06, 21.0361,  96.7366, 15.0,TRUE  ,ISC      ,MS    , 7.5,PAS      ,MS    , 8.0,ABE1     ,mB    , 7.3,AN2      ,Ms    , 7.7,P&S      ,Mw    , 7.7,GUTE     ,UK    , 8.0\"\\n',\n",
       " '\"   914027,ke  ,ISS      ,1913-01-08,19:17:03   , 22.5000, 117.5000,     ,      ,BJI      ,Ms    , 6.5\"\\n',\n",
       " '\"   914108,ke  ,ISC      ,1913-12-21,15:37:56.50, 24.2449, 102.2274, 15.0,TRUE  ,ISC      ,MS    , 6.6,P&S      ,Ms    , 7.1,AN1      ,Ms    , 7.2,P&S      ,Mw    , 7.2,B&D      ,UK    , 7.2\"\\n',\n",
       " '\"   913976,ke  ,GUTE     ,1914-03-28,10:44:48   , 25.0000,  99.0000,100.0,      ,PAS      ,MS    , 6.9,BJI      ,Ms    , 6.6,GUTE     ,UK    , 6.7\"\\n',\n",
       " '\"   913954,ke  ,ISC      ,1915-12-03,02:39:24.54, 27.6997,  91.6401, 15.0,TRUE  ,ISC      ,MS    , 6.5,AN1      ,Ms    , 7.0,BJI      ,Ms    , 7.0,B&D      ,UK    , 7.1\"\\n',\n",
       " '\"   913446,ke  ,ISS      ,1917-04-12,02:54:35   , 18.0000,  97.0000,     ,      ,         ,      ,\"\\n',\n",
       " '\"   913196,ke  ,ISC      ,1918-07-08,10:22:08.90, 24.5990,  90.8528, 15.0,TRUE  ,ISC      ,MS    , 7.2,PAS      ,MS    , 7.6,ABE1     ,mB    , 7.6,P&S      ,Ms    , 7.4,ABE1     ,Ms    , 7.6,P&S      ,Mw    , 7.5,B&D      ,UK    , 7.6\"\\n',\n",
       " '\"   912923,ke  ,ISS      ,1919-09-08,04:08:00   , 18.0000,  97.0000,     ,      ,         ,      ,\"\\n',\n",
       " '\"   912579,ke  ,ISS      ,1920-08-15,06:59:08   , 22.2000,  93.2000,     ,      ,         ,      ,\"\\n',\n",
       " '\"   912194,ke  ,ISS      ,1921-03-30,10:26:30   , 22.2000,  93.2000,     ,      ,         ,      ,\"\\n',\n",
       " '\"   912311,ke  ,ISS      ,1921-09-12,05:09:48   , 18.0000,  97.0000,     ,      ,         ,      ,\"\\n',\n",
       " '\"   911885,ke  ,ISC      ,1922-05-02,11:10:53.44, 20.0485,  98.2534, 35.0,TRUE  ,ISC      ,MS    , 6.7,\"\\n',\n",
       " '\"   912124,ke  ,ISS      ,1922-12-24,00:06:26   , 21.0000,  97.0000,     ,      ,         ,      ,\"\\n',\n",
       " '\"   911420,ke  ,ISC      ,1923-06-22,06:44:40.31, 22.8451,  98.7111, 25.0,TRUE  ,ISC      ,MS    , 7.3,PAS      ,MS    , 7.3,ABE1     ,mB    , 6.3,P&S      ,Ms    , 7.1,ABE1     ,Ms    , 7.3,P&S      ,Mw    , 7.2,B&D      ,UK    , 7.3\"\\n',\n",
       " '\"   911421,ke  ,ISS      ,1923-06-22,12:06:04   , 22.7000,  99.0000,     ,      ,         ,      ,\"\\n',\n",
       " '\"   911495,ke  ,ISS      ,1923-08-10,15:58:06   , 22.6000,  93.4000,     ,      ,         ,      ,\"\\n',\n",
       " '\"   911634,ke  ,ISC      ,1923-09-09,22:03:46.70, 24.9372,  90.7207, 15.0,TRUE  ,ISC      ,MS    , 6.9,PAS      ,MS    , 7.1,ABE1     ,Ms    , 7.1,GUTE     ,UK    , 7.1,B&D      ,UK    , 7.1\"\\n',\n",
       " '\"   910822,ke  ,ISS      ,1924-01-30,00:05:24   , 25.0000,  93.0000,     ,      ,         ,      ,\"\\n',\n",
       " '\"   910834,ke  ,ISS      ,1924-02-14,18:55:30   , 26.0000,  96.0000,     ,      ,         ,      ,\"\\n',\n",
       " '\"   911047,ke  ,ISS      ,1924-08-01,14:42:56   , 26.0000,  96.0000,     ,      ,         ,      ,\"\\n',\n",
       " '\"   911078,ke  ,ISS      ,1924-09-02,02:03:00   , 23.0000,  95.0000,     ,      ,         ,      ,\"\\n',\n",
       " '\"   910712,ke  ,ISS      ,1925-10-14,17:05:18   , 27.0000, 100.0000,     ,      ,         ,      ,\"\\n',\n",
       " '\"   910714,ke  ,ISS      ,1925-10-15,12:36:12   , 27.0000, 100.0000,     ,      ,         ,      ,\"\\n',\n",
       " '\"   909833,ke  ,GUTE     ,1926-05-10,08:19:10   , 26.0000,  97.0000, 80.0,      ,PAS      ,MS    , 6.2,\"\\n',\n",
       " '\"   909854,ke  ,GUTE     ,1926-05-29,22:37:32   , 15.0000,  92.0000, 35.0,TRUE  ,PAS      ,MS    , 5.6,\"\\n',\n",
       " '\"   909933,ke  ,ISS      ,1926-07-12,22:12:30   , 15.5000,  92.5000,     ,      ,         ,      ,\"\\n',\n",
       " '\"   910004,ke  ,ISS      ,1926-08-06,13:17:48   , 26.0000,  96.0000,     ,      ,         ,      ,\"\\n']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display more lines to understand the structure and find where the actual data starts\n",
    "raw_data[10:60]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed49ffdf-bc85-4ded-ad2e-0b2346989667",
   "metadata": {},
   "source": [
    "The data starts after several lines of metadata and introductory text. The actual data seems to start from the line that begins with \"EVENTID,TYPE,AUTHOR,...\". This line indicates the column headers, followed by the earthquake event data.\n",
    "\n",
    "Let's clean and restructure the columns based on this observation.\n",
    "\n",
    "I'll extract the column headers and the subsequent data lines correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "62063fbe-9874-4812-ac22-781b8802f60f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "12 columns passed, passed data had 123 columns",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/internals/construction.py:939\u001b[0m, in \u001b[0;36m_finalize_columns_and_data\u001b[0;34m(content, columns, dtype)\u001b[0m\n\u001b[1;32m    938\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 939\u001b[0m     columns \u001b[38;5;241m=\u001b[39m _validate_or_indexify_columns(contents, columns)\n\u001b[1;32m    940\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    941\u001b[0m     \u001b[38;5;66;03m# GH#26429 do not raise user-facing AssertionError\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/internals/construction.py:986\u001b[0m, in \u001b[0;36m_validate_or_indexify_columns\u001b[0;34m(content, columns)\u001b[0m\n\u001b[1;32m    984\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_mi_list \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(columns) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(content):  \u001b[38;5;66;03m# pragma: no cover\u001b[39;00m\n\u001b[1;32m    985\u001b[0m     \u001b[38;5;66;03m# caller's responsibility to check for this...\u001b[39;00m\n\u001b[0;32m--> 986\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[1;32m    987\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(columns)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m columns passed, passed data had \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    988\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(content)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m columns\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    989\u001b[0m     )\n\u001b[1;32m    990\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_mi_list:\n\u001b[1;32m    991\u001b[0m     \u001b[38;5;66;03m# check if nested list column, length of each sub-list should be equal\u001b[39;00m\n",
      "\u001b[0;31mAssertionError\u001b[0m: 12 columns passed, passed data had 123 columns",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 18\u001b[0m\n\u001b[1;32m     15\u001b[0m data \u001b[38;5;241m=\u001b[39m [line\u001b[38;5;241m.\u001b[39mstrip()\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m data_lines \u001b[38;5;28;01mif\u001b[39;00m line\u001b[38;5;241m.\u001b[39mstrip()]\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Create a DataFrame from the extracted data\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m earthquake_data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(data, columns\u001b[38;5;241m=\u001b[39mheader)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Display the cleaned DataFrame to the user\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mace_tools\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtools\u001b[39;00m; tools\u001b[38;5;241m.\u001b[39mdisplay_dataframe_to_user(name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCleaned Earthquake Data\u001b[39m\u001b[38;5;124m\"\u001b[39m, dataframe\u001b[38;5;241m=\u001b[39mearthquake_data)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/frame.py:806\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    804\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m columns \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    805\u001b[0m         columns \u001b[38;5;241m=\u001b[39m ensure_index(columns)\n\u001b[0;32m--> 806\u001b[0m     arrays, columns, index \u001b[38;5;241m=\u001b[39m nested_data_to_arrays(\n\u001b[1;32m    807\u001b[0m         \u001b[38;5;66;03m# error: Argument 3 to \"nested_data_to_arrays\" has incompatible\u001b[39;00m\n\u001b[1;32m    808\u001b[0m         \u001b[38;5;66;03m# type \"Optional[Collection[Any]]\"; expected \"Optional[Index]\"\u001b[39;00m\n\u001b[1;32m    809\u001b[0m         data,\n\u001b[1;32m    810\u001b[0m         columns,\n\u001b[1;32m    811\u001b[0m         index,  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    812\u001b[0m         dtype,\n\u001b[1;32m    813\u001b[0m     )\n\u001b[1;32m    814\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m arrays_to_mgr(\n\u001b[1;32m    815\u001b[0m         arrays,\n\u001b[1;32m    816\u001b[0m         columns,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    819\u001b[0m         typ\u001b[38;5;241m=\u001b[39mmanager,\n\u001b[1;32m    820\u001b[0m     )\n\u001b[1;32m    821\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/internals/construction.py:520\u001b[0m, in \u001b[0;36mnested_data_to_arrays\u001b[0;34m(data, columns, index, dtype)\u001b[0m\n\u001b[1;32m    517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_named_tuple(data[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;129;01mand\u001b[39;00m columns \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    518\u001b[0m     columns \u001b[38;5;241m=\u001b[39m ensure_index(data[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39m_fields)\n\u001b[0;32m--> 520\u001b[0m arrays, columns \u001b[38;5;241m=\u001b[39m to_arrays(data, columns, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[1;32m    521\u001b[0m columns \u001b[38;5;241m=\u001b[39m ensure_index(columns)\n\u001b[1;32m    523\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/internals/construction.py:845\u001b[0m, in \u001b[0;36mto_arrays\u001b[0;34m(data, columns, dtype)\u001b[0m\n\u001b[1;32m    842\u001b[0m     data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mtuple\u001b[39m(x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m data]\n\u001b[1;32m    843\u001b[0m     arr \u001b[38;5;241m=\u001b[39m _list_to_arrays(data)\n\u001b[0;32m--> 845\u001b[0m content, columns \u001b[38;5;241m=\u001b[39m _finalize_columns_and_data(arr, columns, dtype)\n\u001b[1;32m    846\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m content, columns\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/internals/construction.py:942\u001b[0m, in \u001b[0;36m_finalize_columns_and_data\u001b[0;34m(content, columns, dtype)\u001b[0m\n\u001b[1;32m    939\u001b[0m     columns \u001b[38;5;241m=\u001b[39m _validate_or_indexify_columns(contents, columns)\n\u001b[1;32m    940\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    941\u001b[0m     \u001b[38;5;66;03m# GH#26429 do not raise user-facing AssertionError\u001b[39;00m\n\u001b[0;32m--> 942\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(err) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m    944\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(contents) \u001b[38;5;129;01mand\u001b[39;00m contents[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m np\u001b[38;5;241m.\u001b[39mobject_:\n\u001b[1;32m    945\u001b[0m     contents \u001b[38;5;241m=\u001b[39m convert_object_array(contents, dtype\u001b[38;5;241m=\u001b[39mdtype)\n",
      "\u001b[0;31mValueError\u001b[0m: 12 columns passed, passed data had 123 columns"
     ]
    }
   ],
   "source": [
    "# Identify the correct start index for the actual data\n",
    "data_start_index = None\n",
    "for index, line in enumerate(raw_data):\n",
    "    if 'EVENTID' in line and 'TYPE' in line and 'AUTHOR' in line:\n",
    "        data_start_index = index\n",
    "        break\n",
    "\n",
    "# Extract the header and data lines\n",
    "if data_start_index is not None:\n",
    "    header_line = raw_data[data_start_index].strip().replace('\"','')\n",
    "    data_lines = raw_data[data_start_index + 1:]\n",
    "\n",
    "    # Clean and split the header and data lines into columns\n",
    "    header = [col.strip() for col in header_line.split(',')]\n",
    "    data = [line.strip().replace('\"','').split(',') for line in data_lines if line.strip()]\n",
    "\n",
    "    # Create a DataFrame from the extracted data\n",
    "    earthquake_data = pd.DataFrame(data, columns=header)\n",
    "\n",
    "    # Display the cleaned DataFrame to the user\n",
    "    import ace_tools as tools; tools.display_dataframe_to_user(name=\"Cleaned Earthquake Data\", dataframe=earthquake_data)\n",
    "else:\n",
    "    earthquake_data = pd.DataFrame()\n",
    "    \n",
    "earthquake_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d7e164-582b-431e-8c83-7a302eb30dfb",
   "metadata": {},
   "source": [
    "The data lines have an inconsistent number of columns, possibly due to the presence of commas within the values or additional formatting issues.\n",
    "\n",
    "To handle this, I'll adjust the code to better parse the data lines by accounting for possible issues like extra commas within values.\n",
    "\n",
    "Let's reprocess the data with improved parsing logic. ​"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b8dbcdd6-1870-4569-9b19-8d02d23c928b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "12 columns passed, passed data had 123 columns",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/internals/construction.py:939\u001b[0m, in \u001b[0;36m_finalize_columns_and_data\u001b[0;34m(content, columns, dtype)\u001b[0m\n\u001b[1;32m    938\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 939\u001b[0m     columns \u001b[38;5;241m=\u001b[39m _validate_or_indexify_columns(contents, columns)\n\u001b[1;32m    940\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    941\u001b[0m     \u001b[38;5;66;03m# GH#26429 do not raise user-facing AssertionError\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/internals/construction.py:986\u001b[0m, in \u001b[0;36m_validate_or_indexify_columns\u001b[0;34m(content, columns)\u001b[0m\n\u001b[1;32m    984\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_mi_list \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(columns) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(content):  \u001b[38;5;66;03m# pragma: no cover\u001b[39;00m\n\u001b[1;32m    985\u001b[0m     \u001b[38;5;66;03m# caller's responsibility to check for this...\u001b[39;00m\n\u001b[0;32m--> 986\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[1;32m    987\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(columns)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m columns passed, passed data had \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    988\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(content)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m columns\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    989\u001b[0m     )\n\u001b[1;32m    990\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_mi_list:\n\u001b[1;32m    991\u001b[0m     \u001b[38;5;66;03m# check if nested list column, length of each sub-list should be equal\u001b[39;00m\n",
      "\u001b[0;31mAssertionError\u001b[0m: 12 columns passed, passed data had 123 columns",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 25\u001b[0m\n\u001b[1;32m     22\u001b[0m         data\u001b[38;5;241m.\u001b[39mextend(\u001b[38;5;28mlist\u001b[39m(reader))\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Create a DataFrame from the extracted data\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m earthquake_data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(data, columns\u001b[38;5;241m=\u001b[39mheader)\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# Display the cleaned DataFrame to the user\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mace_tools\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtools\u001b[39;00m; tools\u001b[38;5;241m.\u001b[39mdisplay_dataframe_to_user(name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCleaned Earthquake Data\u001b[39m\u001b[38;5;124m\"\u001b[39m, dataframe\u001b[38;5;241m=\u001b[39mearthquake_data)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/frame.py:806\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    804\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m columns \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    805\u001b[0m         columns \u001b[38;5;241m=\u001b[39m ensure_index(columns)\n\u001b[0;32m--> 806\u001b[0m     arrays, columns, index \u001b[38;5;241m=\u001b[39m nested_data_to_arrays(\n\u001b[1;32m    807\u001b[0m         \u001b[38;5;66;03m# error: Argument 3 to \"nested_data_to_arrays\" has incompatible\u001b[39;00m\n\u001b[1;32m    808\u001b[0m         \u001b[38;5;66;03m# type \"Optional[Collection[Any]]\"; expected \"Optional[Index]\"\u001b[39;00m\n\u001b[1;32m    809\u001b[0m         data,\n\u001b[1;32m    810\u001b[0m         columns,\n\u001b[1;32m    811\u001b[0m         index,  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    812\u001b[0m         dtype,\n\u001b[1;32m    813\u001b[0m     )\n\u001b[1;32m    814\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m arrays_to_mgr(\n\u001b[1;32m    815\u001b[0m         arrays,\n\u001b[1;32m    816\u001b[0m         columns,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    819\u001b[0m         typ\u001b[38;5;241m=\u001b[39mmanager,\n\u001b[1;32m    820\u001b[0m     )\n\u001b[1;32m    821\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/internals/construction.py:520\u001b[0m, in \u001b[0;36mnested_data_to_arrays\u001b[0;34m(data, columns, index, dtype)\u001b[0m\n\u001b[1;32m    517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_named_tuple(data[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;129;01mand\u001b[39;00m columns \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    518\u001b[0m     columns \u001b[38;5;241m=\u001b[39m ensure_index(data[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39m_fields)\n\u001b[0;32m--> 520\u001b[0m arrays, columns \u001b[38;5;241m=\u001b[39m to_arrays(data, columns, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[1;32m    521\u001b[0m columns \u001b[38;5;241m=\u001b[39m ensure_index(columns)\n\u001b[1;32m    523\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/internals/construction.py:845\u001b[0m, in \u001b[0;36mto_arrays\u001b[0;34m(data, columns, dtype)\u001b[0m\n\u001b[1;32m    842\u001b[0m     data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mtuple\u001b[39m(x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m data]\n\u001b[1;32m    843\u001b[0m     arr \u001b[38;5;241m=\u001b[39m _list_to_arrays(data)\n\u001b[0;32m--> 845\u001b[0m content, columns \u001b[38;5;241m=\u001b[39m _finalize_columns_and_data(arr, columns, dtype)\n\u001b[1;32m    846\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m content, columns\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/internals/construction.py:942\u001b[0m, in \u001b[0;36m_finalize_columns_and_data\u001b[0;34m(content, columns, dtype)\u001b[0m\n\u001b[1;32m    939\u001b[0m     columns \u001b[38;5;241m=\u001b[39m _validate_or_indexify_columns(contents, columns)\n\u001b[1;32m    940\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    941\u001b[0m     \u001b[38;5;66;03m# GH#26429 do not raise user-facing AssertionError\u001b[39;00m\n\u001b[0;32m--> 942\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(err) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m    944\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(contents) \u001b[38;5;129;01mand\u001b[39;00m contents[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m np\u001b[38;5;241m.\u001b[39mobject_:\n\u001b[1;32m    945\u001b[0m     contents \u001b[38;5;241m=\u001b[39m convert_object_array(contents, dtype\u001b[38;5;241m=\u001b[39mdtype)\n",
      "\u001b[0;31mValueError\u001b[0m: 12 columns passed, passed data had 123 columns"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "# Identify the correct start index for the actual data\n",
    "data_start_index = None\n",
    "for index, line in enumerate(raw_data):\n",
    "    if 'EVENTID' in line and 'TYPE' in line and 'AUTHOR' in line:\n",
    "        data_start_index = index\n",
    "        break\n",
    "\n",
    "# Extract the header and data lines\n",
    "if data_start_index is not None:\n",
    "    header_line = raw_data[data_start_index].strip().replace('\"','')\n",
    "    data_lines = raw_data[data_start_index + 1:]\n",
    "\n",
    "    # Clean and split the header and data lines into columns\n",
    "    header = [col.strip() for col in header_line.split(',')]\n",
    "    data = []\n",
    "    \n",
    "    for line in data_lines:\n",
    "        if line.strip():\n",
    "            reader = csv.reader([line.strip().replace('\"','')])\n",
    "            data.extend(list(reader))\n",
    "    \n",
    "    # Create a DataFrame from the extracted data\n",
    "    earthquake_data = pd.DataFrame(data, columns=header)\n",
    "    \n",
    "    # Display the cleaned DataFrame to the user\n",
    "    import ace_tools as tools; tools.display_dataframe_to_user(name=\"Cleaned Earthquake Data\", dataframe=earthquake_data)\n",
    "else:\n",
    "    earthquake_data = pd.DataFrame()\n",
    "\n",
    "earthquake_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0dadd65-9c59-4af3-8ae0-1b5ace475676",
   "metadata": {},
   "source": [
    "It appears there are still inconsistencies in the data, likely due to varying numbers of columns in different rows. I will take a more manual approach to identify and parse rows with the correct number of columns.\n",
    "\n",
    "Let's inspect the rows manually to find out where the inconsistencies lie and adjust accordingly. ​"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f625e26a-c3e0-40c7-a482-89ebf391bfb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18 [' 16957826', 'ke  ', 'ISC      ', '1904-08-30', '11:43:37.59', ' 25.5166', '  97.0211', ' 15.0', 'TRUE  ', 'ABE1     ', 'mB    ', ' 7.6', 'AN2      ', 'Ms    ', ' 6.8', 'BJI      ', 'Ms    ', ' 6.8']\n",
      "21 ['   914281', 'ke  ', 'GUTE     ', '1906-08-31', '14:57:30   ', ' 27.0000', '  97.0000', '100.0', '      ', 'PAS      ', 'MS    ', ' 7.0', 'ABE1     ', 'mB    ', ' 6.7', 'GUTE     ', 'UK    ', ' 7.0', 'B&D      ', 'UK    ', ' 7.0']\n",
      "18 [' 16957984', 'ke  ', 'ISC      ', '1908-02-09', '18:13:17.58', ' 29.2781', ' 100.6556', ' 15.0', 'TRUE  ', 'ISC      ', 'MS    ', ' 6.7', 'BJI      ', 'Ms    ', ' 7.1', 'B&D      ', 'UK    ', ' 7.1']\n",
      "27 [' 16958007', 'ke  ', 'ISC      ', '1908-12-12', '12:54:57.27', ' 26.9477', '  96.7734', ' 15.0', 'TRUE  ', 'ISC      ', 'MS    ', ' 7.0', 'PAS      ', 'MS    ', ' 7.5', 'ABE1     ', 'mB    ', ' 7.6', 'AN2      ', 'Ms    ', ' 7.0', 'BJI      ', 'Ms    ', ' 7.0', 'GUTE     ', 'UK    ', ' 7.5']\n",
      "27 [' 16958191', 'ke  ', 'ISC      ', '1912-05-23', '02:24:04.06', ' 21.0361', '  96.7366', ' 15.0', 'TRUE  ', 'ISC      ', 'MS    ', ' 7.5', 'PAS      ', 'MS    ', ' 8.0', 'ABE1     ', 'mB    ', ' 7.3', 'AN2      ', 'Ms    ', ' 7.7', 'P&S      ', 'Mw    ', ' 7.7', 'GUTE     ', 'UK    ', ' 8.0']\n",
      "12 ['   914027', 'ke  ', 'ISS      ', '1913-01-08', '19:17:03   ', ' 22.5000', ' 117.5000', '     ', '      ', 'BJI      ', 'Ms    ', ' 6.5']\n",
      "24 ['   914108', 'ke  ', 'ISC      ', '1913-12-21', '15:37:56.50', ' 24.2449', ' 102.2274', ' 15.0', 'TRUE  ', 'ISC      ', 'MS    ', ' 6.6', 'P&S      ', 'Ms    ', ' 7.1', 'AN1      ', 'Ms    ', ' 7.2', 'P&S      ', 'Mw    ', ' 7.2', 'B&D      ', 'UK    ', ' 7.2']\n",
      "18 ['   913976', 'ke  ', 'GUTE     ', '1914-03-28', '10:44:48   ', ' 25.0000', '  99.0000', '100.0', '      ', 'PAS      ', 'MS    ', ' 6.9', 'BJI      ', 'Ms    ', ' 6.6', 'GUTE     ', 'UK    ', ' 6.7']\n",
      "21 ['   913954', 'ke  ', 'ISC      ', '1915-12-03', '02:39:24.54', ' 27.6997', '  91.6401', ' 15.0', 'TRUE  ', 'ISC      ', 'MS    ', ' 6.5', 'AN1      ', 'Ms    ', ' 7.0', 'BJI      ', 'Ms    ', ' 7.0', 'B&D      ', 'UK    ', ' 7.1']\n",
      "12 ['   913446', 'ke  ', 'ISS      ', '1917-04-12', '02:54:35   ', ' 18.0000', '  97.0000', '     ', '      ', '         ', '      ', '']\n",
      "30 ['   913196', 'ke  ', 'ISC      ', '1918-07-08', '10:22:08.90', ' 24.5990', '  90.8528', ' 15.0', 'TRUE  ', 'ISC      ', 'MS    ', ' 7.2', 'PAS      ', 'MS    ', ' 7.6', 'ABE1     ', 'mB    ', ' 7.6', 'P&S      ', 'Ms    ', ' 7.4', 'ABE1     ', 'Ms    ', ' 7.6', 'P&S      ', 'Mw    ', ' 7.5', 'B&D      ', 'UK    ', ' 7.6']\n",
      "12 ['   912923', 'ke  ', 'ISS      ', '1919-09-08', '04:08:00   ', ' 18.0000', '  97.0000', '     ', '      ', '         ', '      ', '']\n",
      "12 ['   912579', 'ke  ', 'ISS      ', '1920-08-15', '06:59:08   ', ' 22.2000', '  93.2000', '     ', '      ', '         ', '      ', '']\n",
      "12 ['   912194', 'ke  ', 'ISS      ', '1921-03-30', '10:26:30   ', ' 22.2000', '  93.2000', '     ', '      ', '         ', '      ', '']\n",
      "12 ['   912311', 'ke  ', 'ISS      ', '1921-09-12', '05:09:48   ', ' 18.0000', '  97.0000', '     ', '      ', '         ', '      ', '']\n",
      "13 ['   911885', 'ke  ', 'ISC      ', '1922-05-02', '11:10:53.44', ' 20.0485', '  98.2534', ' 35.0', 'TRUE  ', 'ISC      ', 'MS    ', ' 6.7', '']\n",
      "12 ['   912124', 'ke  ', 'ISS      ', '1922-12-24', '00:06:26   ', ' 21.0000', '  97.0000', '     ', '      ', '         ', '      ', '']\n",
      "30 ['   911420', 'ke  ', 'ISC      ', '1923-06-22', '06:44:40.31', ' 22.8451', '  98.7111', ' 25.0', 'TRUE  ', 'ISC      ', 'MS    ', ' 7.3', 'PAS      ', 'MS    ', ' 7.3', 'ABE1     ', 'mB    ', ' 6.3', 'P&S      ', 'Ms    ', ' 7.1', 'ABE1     ', 'Ms    ', ' 7.3', 'P&S      ', 'Mw    ', ' 7.2', 'B&D      ', 'UK    ', ' 7.3']\n",
      "12 ['   911421', 'ke  ', 'ISS      ', '1923-06-22', '12:06:04   ', ' 22.7000', '  99.0000', '     ', '      ', '         ', '      ', '']\n",
      "12 ['   911495', 'ke  ', 'ISS      ', '1923-08-10', '15:58:06   ', ' 22.6000', '  93.4000', '     ', '      ', '         ', '      ', '']\n"
     ]
    }
   ],
   "source": [
    "# Manually inspect a few rows to understand the inconsistencies\n",
    "for line in data_lines[:20]:\n",
    "    print(len(line.strip().replace('\"','').split(',')), line.strip().replace('\"','').split(','))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c309d20-f798-47c2-a841-1705bf2dc476",
   "metadata": {},
   "source": [
    "The data lines have inconsistent lengths, meaning some rows have more columns than others. To handle this, I will:\n",
    "\n",
    "Identify and normalize the maximum number of columns found in any row.\n",
    "Adjust the header and data accordingly to handle these variations.\n",
    "Let's process the data again, ensuring we handle rows with varying numbers of columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7f3f4976-25f0-43b4-ad7a-0a86fe84b3d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EVENTID</th>\n",
       "      <th>TYPE</th>\n",
       "      <th>AUTHOR</th>\n",
       "      <th>DATE</th>\n",
       "      <th>TIME</th>\n",
       "      <th>LAT</th>\n",
       "      <th>LON</th>\n",
       "      <th>DEPTH</th>\n",
       "      <th>DEPFIX</th>\n",
       "      <th>AUTHOR</th>\n",
       "      <th>...</th>\n",
       "      <th>Extra_101</th>\n",
       "      <th>Extra_102</th>\n",
       "      <th>Extra_103</th>\n",
       "      <th>Extra_104</th>\n",
       "      <th>Extra_105</th>\n",
       "      <th>Extra_106</th>\n",
       "      <th>Extra_107</th>\n",
       "      <th>Extra_108</th>\n",
       "      <th>Extra_109</th>\n",
       "      <th>Extra_110</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16957826</td>\n",
       "      <td>ke</td>\n",
       "      <td>ISC</td>\n",
       "      <td>1904-08-30</td>\n",
       "      <td>11:43:37.59</td>\n",
       "      <td>25.5166</td>\n",
       "      <td>97.0211</td>\n",
       "      <td>15.0</td>\n",
       "      <td>TRUE</td>\n",
       "      <td>ABE1</td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>914281</td>\n",
       "      <td>ke</td>\n",
       "      <td>GUTE</td>\n",
       "      <td>1906-08-31</td>\n",
       "      <td>14:57:30</td>\n",
       "      <td>27.0000</td>\n",
       "      <td>97.0000</td>\n",
       "      <td>100.0</td>\n",
       "      <td></td>\n",
       "      <td>PAS</td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16957984</td>\n",
       "      <td>ke</td>\n",
       "      <td>ISC</td>\n",
       "      <td>1908-02-09</td>\n",
       "      <td>18:13:17.58</td>\n",
       "      <td>29.2781</td>\n",
       "      <td>100.6556</td>\n",
       "      <td>15.0</td>\n",
       "      <td>TRUE</td>\n",
       "      <td>ISC</td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16958007</td>\n",
       "      <td>ke</td>\n",
       "      <td>ISC</td>\n",
       "      <td>1908-12-12</td>\n",
       "      <td>12:54:57.27</td>\n",
       "      <td>26.9477</td>\n",
       "      <td>96.7734</td>\n",
       "      <td>15.0</td>\n",
       "      <td>TRUE</td>\n",
       "      <td>ISC</td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>16958191</td>\n",
       "      <td>ke</td>\n",
       "      <td>ISC</td>\n",
       "      <td>1912-05-23</td>\n",
       "      <td>02:24:04.06</td>\n",
       "      <td>21.0361</td>\n",
       "      <td>96.7366</td>\n",
       "      <td>15.0</td>\n",
       "      <td>TRUE</td>\n",
       "      <td>ISC</td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 123 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     EVENTID  TYPE     AUTHOR        DATE         TIME       LAT        LON  \\\n",
       "0   16957826  ke    ISC        1904-08-30  11:43:37.59   25.5166    97.0211   \n",
       "1     914281  ke    GUTE       1906-08-31  14:57:30      27.0000    97.0000   \n",
       "2   16957984  ke    ISC        1908-02-09  18:13:17.58   29.2781   100.6556   \n",
       "3   16958007  ke    ISC        1908-12-12  12:54:57.27   26.9477    96.7734   \n",
       "4   16958191  ke    ISC        1912-05-23  02:24:04.06   21.0361    96.7366   \n",
       "\n",
       "   DEPTH  DEPFIX     AUTHOR  ... Extra_101 Extra_102 Extra_103 Extra_104  \\\n",
       "0   15.0  TRUE    ABE1       ...                                           \n",
       "1  100.0          PAS        ...                                           \n",
       "2   15.0  TRUE    ISC        ...                                           \n",
       "3   15.0  TRUE    ISC        ...                                           \n",
       "4   15.0  TRUE    ISC        ...                                           \n",
       "\n",
       "  Extra_105 Extra_106 Extra_107 Extra_108 Extra_109 Extra_110  \n",
       "0                                                              \n",
       "1                                                              \n",
       "2                                                              \n",
       "3                                                              \n",
       "4                                                              \n",
       "\n",
       "[5 rows x 123 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Determine the maximum number of columns in the data\n",
    "max_columns = max(len(line.strip().replace('\"','').split(',')) for line in data_lines)\n",
    "\n",
    "# Create a new header with the maximum number of columns\n",
    "header_line = raw_data[data_start_index].strip().replace('\"','')\n",
    "header = [col.strip() for col in header_line.split(',')]\n",
    "header += [f'Extra_{i}' for i in range(max_columns - len(header))]\n",
    "\n",
    "# Clean and split the data lines into columns, padding with empty strings for missing values\n",
    "data = []\n",
    "for line in data_lines:\n",
    "    if line.strip():\n",
    "        reader = csv.reader([line.strip().replace('\"','')])\n",
    "        row = list(reader)[0]\n",
    "        row += [''] * (max_columns - len(row))  # Pad the row with empty strings if needed\n",
    "        data.append(row)\n",
    "\n",
    "# Create a DataFrame from the extracted data\n",
    "earthquake_data = pd.DataFrame(data, columns=header)\n",
    "\n",
    "# Display the cleaned DataFrame to the user\n",
    "# import ace_tools as tools; tools.display_dataframe_to_user(name=\"Cleaned Earthquake Data\", dataframe=earthquake_data)\n",
    "earthquake_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4b923df9-7e6d-4057-a14b-c73e190f9585",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned data saved to ../data/Cleaned_ISC_Data.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EVENTID</th>\n",
       "      <th>TYPE</th>\n",
       "      <th>AUTHOR</th>\n",
       "      <th>DATE</th>\n",
       "      <th>TIME</th>\n",
       "      <th>LAT</th>\n",
       "      <th>LON</th>\n",
       "      <th>DEPTH</th>\n",
       "      <th>DEPFIX</th>\n",
       "      <th>AUTHOR</th>\n",
       "      <th>...</th>\n",
       "      <th>Extra_101</th>\n",
       "      <th>Extra_102</th>\n",
       "      <th>Extra_103</th>\n",
       "      <th>Extra_104</th>\n",
       "      <th>Extra_105</th>\n",
       "      <th>Extra_106</th>\n",
       "      <th>Extra_107</th>\n",
       "      <th>Extra_108</th>\n",
       "      <th>Extra_109</th>\n",
       "      <th>Extra_110</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16957826</td>\n",
       "      <td>ke</td>\n",
       "      <td>ISC</td>\n",
       "      <td>30-08-1904</td>\n",
       "      <td>11:43:37.59</td>\n",
       "      <td>25.5166</td>\n",
       "      <td>97.0211</td>\n",
       "      <td>15.0</td>\n",
       "      <td>TRUE</td>\n",
       "      <td>ABE1</td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>914281</td>\n",
       "      <td>ke</td>\n",
       "      <td>GUTE</td>\n",
       "      <td>31-08-1906</td>\n",
       "      <td>14:57:30</td>\n",
       "      <td>27.0000</td>\n",
       "      <td>97.0000</td>\n",
       "      <td>100.0</td>\n",
       "      <td></td>\n",
       "      <td>PAS</td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16957984</td>\n",
       "      <td>ke</td>\n",
       "      <td>ISC</td>\n",
       "      <td>09-02-1908</td>\n",
       "      <td>18:13:17.58</td>\n",
       "      <td>29.2781</td>\n",
       "      <td>100.6556</td>\n",
       "      <td>15.0</td>\n",
       "      <td>TRUE</td>\n",
       "      <td>ISC</td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16958007</td>\n",
       "      <td>ke</td>\n",
       "      <td>ISC</td>\n",
       "      <td>12-12-1908</td>\n",
       "      <td>12:54:57.27</td>\n",
       "      <td>26.9477</td>\n",
       "      <td>96.7734</td>\n",
       "      <td>15.0</td>\n",
       "      <td>TRUE</td>\n",
       "      <td>ISC</td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>16958191</td>\n",
       "      <td>ke</td>\n",
       "      <td>ISC</td>\n",
       "      <td>23-05-1912</td>\n",
       "      <td>02:24:04.06</td>\n",
       "      <td>21.0361</td>\n",
       "      <td>96.7366</td>\n",
       "      <td>15.0</td>\n",
       "      <td>TRUE</td>\n",
       "      <td>ISC</td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 123 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     EVENTID  TYPE     AUTHOR        DATE         TIME       LAT        LON  \\\n",
       "0   16957826  ke    ISC        30-08-1904  11:43:37.59   25.5166    97.0211   \n",
       "1     914281  ke    GUTE       31-08-1906  14:57:30      27.0000    97.0000   \n",
       "2   16957984  ke    ISC        09-02-1908  18:13:17.58   29.2781   100.6556   \n",
       "3   16958007  ke    ISC        12-12-1908  12:54:57.27   26.9477    96.7734   \n",
       "4   16958191  ke    ISC        23-05-1912  02:24:04.06   21.0361    96.7366   \n",
       "\n",
       "   DEPTH  DEPFIX     AUTHOR  ... Extra_101  Extra_102 Extra_103 Extra_104  \\\n",
       "0   15.0  TRUE    ABE1       ...                                            \n",
       "1  100.0          PAS        ...                                            \n",
       "2   15.0  TRUE    ISC        ...                                            \n",
       "3   15.0  TRUE    ISC        ...                                            \n",
       "4   15.0  TRUE    ISC        ...                                            \n",
       "\n",
       "  Extra_105 Extra_106 Extra_107 Extra_108 Extra_109 Extra_110  \n",
       "0                                                              \n",
       "1                                                              \n",
       "2                                                              \n",
       "3                                                              \n",
       "4                                                              \n",
       "\n",
       "[5 rows x 123 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "from datetime import datetime\n",
    "\n",
    "# Load the uploaded CSV file\n",
    "file_path = '../data/raw_ISC_catalog.csv'\n",
    "\n",
    "# Read the raw data\n",
    "with open(file_path, 'r', encoding='latin1') as file:\n",
    "    raw_data = file.readlines()\n",
    "\n",
    "# Identify the correct start index for the actual data\n",
    "data_start_index = None\n",
    "for index, line in enumerate(raw_data):\n",
    "    if 'EVENTID' in line and 'TYPE' in line and 'AUTHOR' in line:\n",
    "        data_start_index = index\n",
    "        break\n",
    "\n",
    "# Extract the header and data lines\n",
    "if data_start_index is not None:\n",
    "    header_line = raw_data[data_start_index].strip().replace('\"','')\n",
    "    data_lines = raw_data[data_start_index + 1:]\n",
    "\n",
    "    # Clean and split the header and data lines into columns\n",
    "    header = [col.strip() for col in header_line.split(',')]\n",
    "    \n",
    "    # Determine the maximum number of columns in the data\n",
    "    max_columns = max(len(line.strip().replace('\"','').split(',')) for line in data_lines)\n",
    "\n",
    "    # Create a new header with the maximum number of columns\n",
    "    header += [f'Extra_{i}' for i in range(max_columns - len(header))]\n",
    "\n",
    "    # Clean and split the data lines into columns, padding with empty strings for missing values\n",
    "    data = []\n",
    "    for line in data_lines:\n",
    "        if line.strip():\n",
    "            reader = csv.reader([line.strip().replace('\"','')])\n",
    "            row = list(reader)[0]\n",
    "            row += [''] * (max_columns - len(row))  # Pad the row with empty strings if needed\n",
    "            data.append(row)\n",
    "\n",
    "    # Create a DataFrame from the extracted data\n",
    "    earthquake_data = pd.DataFrame(data, columns=header)\n",
    "\n",
    "    # Convert DEPTH and MAG columns to numeric, forcing errors to NaN\n",
    "    earthquake_data['DEPTH'] = pd.to_numeric(earthquake_data['DEPTH'], errors='coerce')\n",
    "    earthquake_data['MAG'] = pd.to_numeric(earthquake_data['MAG'], errors='coerce')\n",
    "\n",
    "    # Drop rows where DEPTH or MAG is NaN\n",
    "    earthquake_data = earthquake_data.dropna(subset=['DEPTH', 'MAG'])\n",
    "\n",
    "    # Function to parse dates flexibly\n",
    "    def parse_date(date_str):\n",
    "        for fmt in ('%d/%m/%Y', '%d-%m-%Y', '%Y-%m-%d', '%d/%m/%y'):\n",
    "            try:\n",
    "                return datetime.strptime(date_str, fmt).strftime('%d-%m-%Y')\n",
    "            except ValueError:\n",
    "                continue\n",
    "        return None\n",
    "\n",
    "    # Apply the date parsing function to the DATE column\n",
    "    earthquake_data['DATE'] = earthquake_data['DATE'].apply(parse_date)\n",
    "\n",
    "    # Drop rows where DATE could not be parsed\n",
    "    earthquake_data = earthquake_data.dropna(subset=['DATE'])\n",
    "\n",
    "else:\n",
    "    earthquake_data = pd.DataFrame()\n",
    "\n",
    "# Save the cleaned DataFrame to a new CSV file\n",
    "output_file_path = '../data/Cleaned_ISC_Data.csv'\n",
    "\n",
    "earthquake_data.to_csv(output_file_path, index=False)\n",
    "\n",
    "print(f\"Cleaned data saved to {output_file_path}\")\n",
    "\n",
    "earthquake_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6cf60928-98ea-472c-a90c-df9e05118691",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned data saved to ../data/Cleaned_ISC_Data2.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EVENTID</th>\n",
       "      <th>TYPE</th>\n",
       "      <th>AUTHOR</th>\n",
       "      <th>DATE</th>\n",
       "      <th>TIME</th>\n",
       "      <th>LAT</th>\n",
       "      <th>LON</th>\n",
       "      <th>DEPTH</th>\n",
       "      <th>DEPFIX</th>\n",
       "      <th>AUTHOR.1</th>\n",
       "      <th>...</th>\n",
       "      <th>Extra_101</th>\n",
       "      <th>Extra_102</th>\n",
       "      <th>Extra_103</th>\n",
       "      <th>Extra_104</th>\n",
       "      <th>Extra_105</th>\n",
       "      <th>Extra_106</th>\n",
       "      <th>Extra_107</th>\n",
       "      <th>Extra_108</th>\n",
       "      <th>Extra_109</th>\n",
       "      <th>Extra_110</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16957826</td>\n",
       "      <td>ke</td>\n",
       "      <td>ISC</td>\n",
       "      <td>30-08-1904</td>\n",
       "      <td>11:43:37.59</td>\n",
       "      <td>25.5166</td>\n",
       "      <td>97.0211</td>\n",
       "      <td>15.0</td>\n",
       "      <td>TRUE</td>\n",
       "      <td>ABE1</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>914281</td>\n",
       "      <td>ke</td>\n",
       "      <td>GUTE</td>\n",
       "      <td>31-08-1906</td>\n",
       "      <td>14:57:30</td>\n",
       "      <td>27.0000</td>\n",
       "      <td>97.0000</td>\n",
       "      <td>100.0</td>\n",
       "      <td></td>\n",
       "      <td>PAS</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16957984</td>\n",
       "      <td>ke</td>\n",
       "      <td>ISC</td>\n",
       "      <td>09-02-1908</td>\n",
       "      <td>18:13:17.58</td>\n",
       "      <td>29.2781</td>\n",
       "      <td>100.6556</td>\n",
       "      <td>15.0</td>\n",
       "      <td>TRUE</td>\n",
       "      <td>ISC</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16958007</td>\n",
       "      <td>ke</td>\n",
       "      <td>ISC</td>\n",
       "      <td>12-12-1908</td>\n",
       "      <td>12:54:57.27</td>\n",
       "      <td>26.9477</td>\n",
       "      <td>96.7734</td>\n",
       "      <td>15.0</td>\n",
       "      <td>TRUE</td>\n",
       "      <td>ISC</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>16958191</td>\n",
       "      <td>ke</td>\n",
       "      <td>ISC</td>\n",
       "      <td>23-05-1912</td>\n",
       "      <td>02:24:04.06</td>\n",
       "      <td>21.0361</td>\n",
       "      <td>96.7366</td>\n",
       "      <td>15.0</td>\n",
       "      <td>TRUE</td>\n",
       "      <td>ISC</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 123 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    EVENTID  TYPE     AUTHOR        DATE         TIME      LAT       LON  \\\n",
       "0  16957826  ke    ISC        30-08-1904  11:43:37.59  25.5166   97.0211   \n",
       "1    914281  ke    GUTE       31-08-1906  14:57:30     27.0000   97.0000   \n",
       "2  16957984  ke    ISC        09-02-1908  18:13:17.58  29.2781  100.6556   \n",
       "3  16958007  ke    ISC        12-12-1908  12:54:57.27  26.9477   96.7734   \n",
       "4  16958191  ke    ISC        23-05-1912  02:24:04.06  21.0361   96.7366   \n",
       "\n",
       "   DEPTH  DEPFIX   AUTHOR.1  ... Extra_101  Extra_102 Extra_103 Extra_104  \\\n",
       "0   15.0  TRUE    ABE1       ...       NaN        NaN       NaN       NaN   \n",
       "1  100.0          PAS        ...       NaN        NaN       NaN       NaN   \n",
       "2   15.0  TRUE    ISC        ...       NaN        NaN       NaN       NaN   \n",
       "3   15.0  TRUE    ISC        ...       NaN        NaN       NaN       NaN   \n",
       "4   15.0  TRUE    ISC        ...       NaN        NaN       NaN       NaN   \n",
       "\n",
       "   Extra_105 Extra_106 Extra_107  Extra_108 Extra_109 Extra_110  \n",
       "0        NaN       NaN       NaN        NaN       NaN       NaN  \n",
       "1        NaN       NaN       NaN        NaN       NaN       NaN  \n",
       "2        NaN       NaN       NaN        NaN       NaN       NaN  \n",
       "3        NaN       NaN       NaN        NaN       NaN       NaN  \n",
       "4        NaN       NaN       NaN        NaN       NaN       NaN  \n",
       "\n",
       "[5 rows x 123 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # import pandas as pd\n",
    "# # from datetime import datetime\n",
    "\n",
    "# # Load the cleaned data file\n",
    "# file_path = '../data/Cleaned_ISC_Data.csv'\n",
    "# earthquake_data = pd.read_csv(file_path)\n",
    "\n",
    "# # Function to parse dates flexibly and convert to DD-MM-YYYY format\n",
    "# def parse_date(date_str):\n",
    "#     for fmt in ('%d/%m/%Y', '%d-%m-%Y', '%Y-%m-%d', '%d/%m/%y'):\n",
    "#         try:\n",
    "#             return datetime.strptime(date_str, fmt).strftime('%d-%m-%Y')\n",
    "#         except ValueError:\n",
    "#             continue\n",
    "#     return None\n",
    "\n",
    "# # Apply the date parsing function to the DATE column\n",
    "# earthquake_data['DATE'] = earthquake_data['DATE'].apply(parse_date)\n",
    "\n",
    "# # Drop rows where DATE could not be parsed\n",
    "# earthquake_data = earthquake_data.dropna(subset=['DATE'])\n",
    "\n",
    "# # Save the cleaned DataFrame with corrected date format to a new CSV file\n",
    "# output_file_path = '../data/Cleaned_ISC_Data2.csv'\n",
    "# earthquake_data.to_csv(output_file_path, index=False)\n",
    "\n",
    "# print(f\"Cleaned data saved to {output_file_path}\")\n",
    "\n",
    "# earthquake_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124c4911-9f21-4426-a17d-ac9bef0af581",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
